# -*- coding: utf-8 -*-
"""121ad0010_ml_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aGZSH5xTSGxtUcSKhE_u4cAxBPURQ85n
"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout
import zipfile

#Unzip the dataset
with zipfile.ZipFile("/content/images.zip", 'r') as zip_ref:
    zip_ref.extractall("emotions_dataset")

# Define the path to your dataset directory
dataset_dir = "emotions_dataset/images/train"

# Define the list of emotion labels
emotion_labels = os.listdir(dataset_dir)

# Create empty lists to store the images and corresponding labels
images = []
labels = []

import os

# Define the path to the dataset's "train" directory
dataset_dir = "emotions_dataset/images/train"  # Adjust the path as needed

# Define the list of emotion labels
emotion_labels = os.listdir(dataset_dir)

# Create a dictionary to store the count of images for each emotion
image_counts = {}

# Iterate through the subfolders for each emotion
for emotion in emotion_labels:
    emotion_dir = os.path.join(dataset_dir, emotion)
    image_files = [f for f in os.listdir(emotion_dir) if os.path.isfile(os.path.join(emotion_dir, f))]
    image_counts[emotion] = len(image_files)

# Print the counts for each emotion
for emotion, count in image_counts.items():
    print(f"Number of images in '{emotion}' directory: {count}")

# Define the number of samples to use from each emotion
samples_per_emotion = 3100

# Iterate through the subfolders for each emotion
for emotion in emotion_labels:
    emotion_dir = os.path.join(dataset_dir, emotion)
    for img_filename in os.listdir(emotion_dir)[:samples_per_emotion]:
        img = cv2.imread(os.path.join(emotion_dir, img_filename), 1)  # Load as color image
        img = cv2.resize(img, (64, 64))  # Resize to 64x64 pixels
        img = img / 255.0  # Normalize pixel values to [0, 1]
         # Introduce noise by adding Gaussian noise
        noise = np.random.normal(0, 0.05, img.shape).astype(np.float32)
        noisy_img = img + noise # adding noise to image to increase accurat
        images.append(noisy_img)
        labels.append(emotion_labels.index(emotion))

# Convert the lists to NumPy arrays
images = np.array(images)
labels = np.array(labels)

# Split the data into training and testing sets
train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)

# Define the CNN model
model = keras.Sequential()

# Convolutional layers
# Convolutional layers with Leaky ReLU and Batch Normalization
model.add(layers.Conv2D(32, (3, 3), input_shape=(64, 64, 3)))  # Convolutional layer with 32 filters and a 3x3 kernel
model.add(keras.layers.LeakyReLU(alpha=0.2))  # Leaky ReLU activation with alpha parameter
model.add(layers.BatchNormalization())  # Batch Normalization for improving training stability
model.add(layers.MaxPooling2D((2, 2))) # Max pooling with a 2x2 pool size

model.add(layers.Conv2D(64, (3, 3)))  # Convolutional layer with 64 filters and a 3x3 kernel
model.add(keras.layers.LeakyReLU(alpha=0.05))  # Leaky ReLU activation with alpha parameter
model.add(layers.MaxPooling2D((2, 2)) ) # Max pooling

model.add(layers.Conv2D(128, (3, 3))) # Convolutional layer with 128 filters and a 3x3 kernel
model.add(keras.layers.LeakyReLU(alpha=0.01))  # Leaky ReLU activation with alpha parametern
model.add(layers.MaxPooling2D((2, 2)))  # Max pooling

# Flatten the feature maps
model.add(layers.Flatten())

# Fully connected layers
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.5))  # Regularization
model.add(layers.Dense(7, activation='softmax'))  #  7 emotions[angry,sad,disgust,fear,happy,suprise,neutral]

# Define the learning rate
learning_rate = 0.001

# Compile the model with the specified learning rate
optimizer = keras.optimizers.Adam(learning_rate=learning_rate)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',  # Appropriate for integer labels
              metrics=['accuracy'])

# Print a summary of the model's architecture
model.summary()

history = model.fit(train_images, train_labels,
                    epochs=10,  # Adjust the number of epochs as needed
                    batch_size=32,  # Adjust the batch size as needed
                    validation_data=(test_images, test_labels),
                    verbose=1)

# Access training and validation accuracy and loss
train_accuracy = history.history['accuracy']
train_loss = history.history['loss']
val_accuracy = history.history['val_accuracy']
val_loss = history.history['val_loss']

# Print the final training and validation accuracy
final_train_accuracy = train_accuracy[-1]
final_val_accuracy = val_accuracy[-1]
print(f"Final Training Accuracy: {final_train_accuracy:.4f}")
print(f"Final Validation Accuracy: {final_val_accuracy:.4f}")

# Plot training and validation accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')
if 'val_accuracy' in history.history:
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
elif 'val_acc' in history.history:
    plt.plot(history.history['val_acc'], label='Validation Accuracy', color='orange')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='best')

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', color='blue')
if 'val_loss' in history.history:
    plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='best')

plt.show()

# Predict labels for the test set
test_predictions = model.predict(test_images)
test_pred_labels = np.argmax(test_predictions, axis=1)

# Generate the confusion matrix
cm = confusion_matrix(test_labels, test_pred_labels)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_labels, yticklabels=emotion_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Generate a classification report
classification_rep = classification_report(test_labels, test_pred_labels, target_names=emotion_labels)
print(classification_rep)

model.save('model_emotion_recogn.h5')

# Load your trained model
model = keras.models.load_model('/content/model_emotion_recogn.h5')

#Make predictions on the test data
test_predictions = model.predict(test_images)

# Select a random sample of 25 test images
import random
sample_indices = random.sample(range(len(test_images)), 25)
sample_images = test_images[sample_indices]
sample_labels = test_labels[sample_indices]

# Generate predictions for the sampled test images
sample_predictions = test_predictions[sample_indices]
sample_pred_labels = np.argmax(sample_predictions, axis=1)

# Plot the sampled test images along with their true and predicted labels
plt.figure(figsize=(15, 10))
for i in range(len(sample_indices)):
    plt.subplot(5, 5, i + 1)
    plt.imshow(sample_images[i])
    true_label = emotion_labels[sample_labels[i]]
    pred_label = emotion_labels[sample_pred_labels[i]]
    plt.title(f'True: {true_label}\nPredicted: {pred_label}')
    plt.axis('off')

plt.show()

from google.colab.patches import cv2_imshow
# Load and preprocess an input image
image_path = "/content/download.jpeg"
img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Load the image as a color image
image = img
img = cv2.resize(img, (64, 64))  # Resize to 64x64 pixels (adjust if needed)
img = img / 255.0  # Normalize pixel values to [0, 1]
input_image = np.expand_dims(img, axis=0)  # Add a batch dimension

# Perform the prediction
predictions = model.predict(input_image)

# Get the predicted emotion label
predicted_label_index = np.argmax(predictions)
predicted_emotion = emotion_labels[predicted_label_index]

cv2_imshow(image)  # Display the input image
print(f"Predicted Emotion: {predicted_emotion}")